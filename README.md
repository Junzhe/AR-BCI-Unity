Unity-based AR interface for **EEG/BCI-driven target selection** and **robot grasping**.  
The app receives **brain signals via OSC**, lets the user **select and confirm objects in AR**, and then sends the **target ID to a robot controller** (e.g., Raspberry Pi + MyCobot) via HTTP.

åŸºäº Unity çš„ **è„‘æœºæ¥å£ + å¢å¼ºç°å® + æœºæ¢°è‡‚æŠ“å–** é¡¹ç›®ï¼šé€šè¿‡è„‘ç”µæŒ‡ä»¤åœ¨æ‰‹æœº AR ç”»é¢ä¸­é€‰æ‹©/ç¡®è®¤ç›®æ ‡ç‰©ä½“ï¼Œå¹¶å°†ç›®æ ‡ç¼–å·å‘é€ç»™æœºæ¢°è‡‚ç«¯æ‰§è¡ŒæŠ“å–ä»»åŠ¡ã€‚

---

## 1. Project Overview

This project is part of a larger **BCIâ€“ARâ€“Robot** closed-loop system:

1. **BCI / EEG**  
   - Emotiv (or other) EEG headset  
   - Motor imagery commands (e.g., `/left`, `/right`, `/lift`, `/confirm`) encoded as OSC messages

2. **AR-BCI-Unity (this repo)**  
   - Runs on an Android phone  
   - Uses AR (marker / QR / image tracking) to recognize multiple physical objects  
   - Uses EEG commands to move a â€œselection cursorâ€ between objects and confirm the final target  
   - Highlights state:  
     - **Idle** â€“ all targets white  
     - **Selected** â€“ current target turns yellow and scales up  
     - **Confirmed** â€“ target turns green, and its ID is sent to the robot

3. **Robot side (e.g., Raspberry Pi + MyCobot)**  
   - Receives the target ID (A/B/C/â€¦) from the phone via **HTTP** (and/or OSC)  
   - Maps ID â†’ visual marker ID (e.g., STAG / ArUco)  
   - Runs camera-based pose estimation + handâ€“eye calibration  
   - Executes the corresponding **grasping motion**

> In short: **è„‘ç”µä¿¡å· â†’ æ‰‹æœº AR ç›®æ ‡é€‰æ‹© â†’ å‘é€ç›®æ ‡ç¼–å· â†’ æœºæ¢°è‡‚è§†è§‰è¯†åˆ«å¹¶æŠ“å–**ã€‚

---

## 2. Main Features

- **EEG-driven target selection**
  - Use BCI commands to switch between multiple AR targets
  - Final confirmation via a dedicated EEG command (e.g., `/confirm` or `/lift`)

- **AR-based multi-object scene**
  - Plane detection + marker / image tracking
  - Each object is visualized as a 3D arrow / marker
  - Dynamic color + scale to indicate selection state

- **Communication bridge**
  - **OSC** input from BCI â†’ Unity (e.g., from BCI-OSC / Python scripts)
  - **HTTP POST** (or OSC) output from Unity â†’ robot controller
  - Message format e.g.:
    ```json
    { "target": "A" }
    ```

- **Test utilities**
  - `TestFile/test_confirm.py` for sending test `/confirm` signals to Unity when BCI is not available

---

## 3. Repository Structure

At the top level:

```text
AR-BCI-Unity/
â”œâ”€ Assets/           # Unity assets: scenes, prefabs, scripts, materials, etc.
â”œâ”€ Packages/         # Unity packages (AR Foundation, etc.)
â”œâ”€ ProjectSettings/  # Unity project configuration
â”œâ”€ TestFile/
â”‚  â””â”€ test_confirm.py  # Python script to send OSC 'confirm' signal for testing
â”œâ”€ .gitignore
â””â”€ .vsconfig
````

> Note: The detailed scripts and scenes are under `Assets/`. Key components include AR setup, OSC receiver, and HTTP client for robot communication.

---

## 4. Requirements

### Unity & AR

* Unity (2020+; recommended an LTS version)
* AR support:

  * AR Foundation / ARCore XR Plugin (for Android)
  * Properly configured AR scene (camera, session origin, etc.)
* Mobile device:

  * Android phone (e.g., Xiaomi 14) with ARCore support

### BCI / OSC side

* EEG device (e.g., Emotiv EPOC series)
* Software to send OSC:

  * EmotivBCI + BCI-OSC (or any custom Python script)
  * Network reachable from the phone (same Wi-Fi / hotspot)

### Robot side (optional but recommended)

* Raspberry Pi (robot controller)
* Robot arm (e.g., **MyCobot 280Pi** or similar)
* Python 3.x environment
* Flask (or similar HTTP server) to receive target ID, e.g.:

  * `POST /target` with JSON `{ "target": "A" }`
* Vision + calibration code (STAG / ArUco marker detection, handâ€“eye calibration, etc.)

---

## 5. Getting Started

### 5.1 Clone & Open in Unity

```bash
git clone https://github.com/Junzhe/AR-BCI-Unity.git
cd AR-BCI-Unity
```

1. Open the project in Unity Hub.
2. Let Unity import all assets and packages.
3. Open the main AR scene under `Assets/` (e.g., your AR demo scene).

### 5.2 Configure OSC Input (BCI â†’ Unity)

1. In Unity, locate the script / GameObject that receives OSC (e.g., a receiver component).
2. Set:

   * **Listening IP**: usually `0.0.0.0` on the phone
   * **Port**: must match the port used by your BCI OSC sender
3. Configure the expected OSC addresses, for example:

   * `/left`, `/right` â€” switch selected target
   * `/lift` or `/confirm` â€” confirm the current target

#### Testing with `test_confirm.py`

1. Ensure the phone and PC are on the **same network**.
2. In `TestFile/test_confirm.py`, set:

   * `UNITY_IP` â†’ phone IP
   * `UNITY_PORT` â†’ OSC port Unity listens on
3. Run:

   ```bash
   python test_confirm.py
   ```
4. You should see the AR target change to a â€œconfirmedâ€ state in Unity.

### 5.3 Configure HTTP Output (Unity â†’ Robot)

1. In Unity, find the script that sends the target ID to the robot (e.g., HTTP client).

2. Set:

   * Robot controller URL, e.g.: `http://<raspberry_pi_ip>:5000/target`

3. Confirm that the Raspberry Pi is running a Flask server like:

   ```python
   @app.route("/target", methods=["POST"])
   def target():
       data = request.get_json()
       target_code = data.get("target", "A")
       # Map target_code -> STAG/ArUco ID and start grasping
       ...
   ```

4. On EEG confirmation:

   * Unity picks the current target (A/B/Câ€¦)
   * Sends HTTP POST to `/target`
   * Robot executes visual recognition + grasping pipeline

---

## 6. Interaction Logic

**Default visual logic (å¯æŒ‰éœ€ä¿®æ”¹):**

1. **Initialization**

   * All AR targets are spawned as small **white** arrows

2. **Selection (EEG /left, /right, â€¦)**

   * Active target turns **yellow**
   * Scale increases slightly to emphasize focus

3. **Confirmation (EEG /confirm or /lift)**

   * Target turns **green**
   * Unity continuously or once-off sends the target ID to the robot controller
   * Robot starts grasping procedure

This interface is designed for **zero-touch, multimodal interaction**, combining **EEG intent** + **AR visualization** + **robot actions**.

---

## 7. Roadmap / TODO

* [ ] Add screenshots / demo GIFs of the AR interface
* [ ] Provide example Unity scenes and prefabs for quick start
* [ ] Release sample Python code for:

  * BCI OSC sender
  * Flask robot controller + MyCobot grasping
* [ ] Link to associated paper / preprint once published

---

## 8. Acknowledgements

This repository is developed as part of an ongoing research project on **multimodal BCIâ€“ARâ€“Robot collaboration**.
If you use or extend this project for academic research, please consider acknowledging:

> *Junzhe Wang, et al. â€œEEG-Driven ARâ€“Robot Grasping System for Zero-Touch Manipulationâ€ (work in progress).*

---

## 9. Contact

For questions, issues, or collaboration:

* **Author**: Junzhe Wang
* **GitHub**: [@Junzhe](https://github.com/Junzhe)

æ¬¢è¿æ issue æˆ–è€…ç›´æ¥è”ç³»ä½œè€…ï¼Œä¸€èµ·äº¤æµè„‘æœºæ¥å£ + AR + æœºå™¨äººæ–¹å‘çš„ç ”ç©¶ä¸å¼€å‘ ğŸ™Œ
